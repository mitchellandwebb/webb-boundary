
We are wanting to write DSL code for the boundary as follows:

A boundary is defined as 

```
boundary Name where
  function1 :: Int -> Number -> Effect String
  function2 :: Name -> Nullable Height -> Aff Example
```

Type names will always be capitalized. keywords and function names will be entirely in lowercase. To specify whether the bound is syncronous or asyncronous, the final type of a boundary function must be labeled 'Effect' or 'Aff' to signal the intent -- whether the boundary is expected to be blocking to the calling thread or not.

Within a boundary's function, all types should be named. Names can be primitives like Int, Number, Double, Boolean, and String. A type can also be made nullable -- that is, possibly not present -- with a Nullable label. That is the extent of the primitive types. 

Several complex types can be defined, as shown below. The `Array` and `Tuple` types
are predefined and just need type arguments, but to use labeled tuples (records), a 
type alias must be defined, as in `Example` below. This gives us a proper name that we can use in the generated output code, so that the target language can reference this type without needing to determine a name.

```
boundary Name where
  function1 :: Array Int -> Tuple String Int -> Effect Unit
  
type Example = { name :: String, age :: Int }
type Path = String
```

As in the example above, the `type` keyword also allows us to alias existing types to improve readability. If the target language supports aliases, the alias will be preserved; otherwise, the final aliased type will be used.

Just as multiple type aliases can be specified in one file, multiple _boundaries_ can
also be specified in one file. Thus we can have 

```
boundary Person where
  function1 :: Array Int -> Tuple String Int -> Effect Unit
  
boundary Person2 where
  function1 :: Int -> String -> Aff Unit
``` 

Since the boundaries have different names, the function names are allowed to be the same. It is up to code generation to determine how to handle this in the output language, to determine whether this would cause trouble, and to write the output files correctly so that there's no issue.

We also might have _multiple_ boundary files. We don't want that kind of complication. Each boundary file is compiled separately, thank you very much, and does not rely on imports from anywhere else. We'll keep this simple and useful.


----------------------------------------------

Okay. We now have the parse tree as a pure data structure. There's ... both
good and bad in that. We cannot easily modify the tree in this form. On the other
hand, it's well-typed and provides a clear way of extracting information about
what we have, with a visitor that can at least _try_ to visit items consistently.

But okay. We have this initial data structure, and reason to allow it to be stable. What next? What is it that we want to do with it? Well, first we want to validate a few things. 

- We want to build an initial symbol table. We want to know what symbols are already declared and defined in the global scope (the only scope we have, here)
- We want to use the symbol table to check that all symbols are actually _defined_. In our case, there's only the global scope, so that's easy-enough to check, but if we had lexical scopes, we'd want to go into each scope and _build on_ the global lexical scope over time as we go deeper into each lexical scope, to see whether all symbols are defined in each scope.
- We want to use the symbol table to generate knowledge of each symbol's _type_. We might not have all this information initially, but we need to build it up, gradually. In this process, we may need to verify type _completeness_ -- for example, an (Array Array) is no good because the second Array is not a complete type, but an (Array (Array Int)) would be sufficient. This process is thus recursive. We don't necessarily know the dependency order of all types. So we make a best guess and, when we encounter a type within the file, if it depends on other symbols, we type those symbols first. This is an informal graph -- so if we depend on another type, circularly, we detect
that we've visited this type before and err with a circular type.
- Once we've checked that all types are complete, we can output the types, ideally in the same order they appear in the file. We'll output it to a single target file, converting it to the right structure in the target language.

One interesting point, though, is circular types -- or rather, recursive types. How can a type ultimately refer to itself? What does that require? The problem is that when a type refers to itself is it well-typed? Well, that depends. In a simple type like a Record -- _no_ . The type is recursive and thus cannot terminate. Although it is _typable_, it is not constructable except by recursive loop. If the type has _alternatives_ that might terminate, then it is possible. But in theory, nothing is 
wrong with recursive types -- it is, in theory, a valid, terminated type that will simply be impossible to construct at runtime. This differs between Purescript and Kot Kotlin of the implicit _pure references_, and also whether a value is _required_ at runtime -- but it's certainly true that we can obtain subtle construction bugs as a result. Preventing the bare circular type does prevent recursion in a bunch of cases, but naturally, not all.

------------------------------

Okay. So we have the ability to construct the symbol table, and to check the boundaries and their functions for correctness against the existing symbols and their types. So ... now what? How do we go about generating final files? And do we 
even want to do the generation here? What would make the generation pleasant, and how do we know what to generate? How much distance is there from the Tree, or from a Symbol Table, or whatever, to the file that we want to generate? And in what steps should we try to bridge that distance?

It would seem the parse tree itself ... is not sufficient. Type aliases aren't a great way for us to figure what needs to go where. But there is a 1-1 correspondence between alias and construct in most languages. So ... maybe?

After all, what did type-checking accomplish? It told us that the structure of the parse tree also correctly represents intent. But what is the intent that we actually
want to write?

data types
interface types using the data types, expressed in the form of the language.

So we need the types that need to be written, in declaration order, so we can write
systematically. And we need them in a form that is _interpretable_. Aliases may be Int,
Double -- primitives that are easy to convert by looking up the target type in the target
language. But aliases may also be product types or records. Since circularity is not allowed,
we should observe a clear declaration path. We _should_. But when we see product types, how
do we map these to target types in the target language? And ... the symbol table does
specify how symbols refer to each other. But it doesn't really store the target 
type itself; if I say `type A = List Int` and `type B = Map Int Int`, the symbol 
table understands that B is an alias ... but it doesn't understand what _type_ of
alias it is. It doesn't have that data; it doesn't store its actual definition data; and
it ... doesn't necessarily need to, here. So indeed ... what data is actually needed? What
actually ... _is_ an alias?
  An alias refers to a complete type. An alias can use _other_ aliases. Thus, an alias
is a symbol characterized by its complete parameter, NOT any particular single name. Thus,
it is a symbol to _multiple_ other symbols -- one for each parameter within it. In the end, the correctness of an alias is not determined by the alias itself -- if we were doing type _matching_, then yes, an alias would need to look up the other type by itself -- ALL of the other type -- to derive a concrete fully-reduced parameter. But in terms of the alias referring to a valid symbol -- that is unneeded. Parameters can be validated on their own against the symbol table in this case.
  But in the end, there's a funny difference in the symbol table. Aliases refer to
concrete types with the type symbols. But other symbols refer to single symbols on their
own, that map to type-ish definitions. Thus, an Alias like `Array Int` refers to an Int,
while `Array` refers to a `Product 1` -- it does not itself have a complete type, but 
maintains a record of how it is valid to use this symbol, as a higher-kinded type. This
is useful for type-checking. But what we want when writing code like this is _not_ every
incomplete type, which cannot have a concrete representation (at least in our case) at
runtime, but the complete types.
  Even though we _say_ aliases, we really are declaring a type that needs representation. And since it has a path through the symbol table, it's useful -- it's part of how we traverse the
relation of symbols to other symbols quickly, until we bottom out at _base_ symbols and
the information that is part of them. And so that's ... fairly odd, actually. Because we 
don't want the base symbols. We don't want to follow anything anywhere; we don't care if
something is a Product or an Alias or whatever, when we're trying to compile it. What we 
really want is just to look at the Aliases as they really are -- A Name associated with
other names -- and ensure these find their way into the output. An `Array Int`? A
`Map String String`? A `Boolean`? These all find standard definitions and ways of operating
in the target language compiler, and can have access to the SymbolTable if that's useful,
but otherwise it should be enough to write a alias with a name and a construction of the 
output type. 
  But is it actually enough? No, it's not. After all, we might see an `Array (Array Int)`. That's far more than a simple string. So we have the alias an its parameter, and it is
this data that needs to be transformed into the target language. No further processing is 
needed; this is a perfect representation of our knowledge about the alias, when paired
with the Symbol table.
  What we care about with aliases is ensuring that they are, or can be, defined in the right order. They don't have to be; many languages don't care, but some do, like Clojure (if we assume a Typed Clojure). So we need to start with an Array of aliases in the order presented 
in the original file, but provide a _sort_ function for the array, if the target language
desires to sort.
  Similarly, we need to handle Boundaries. Typically we place these _after_ the aliases,
so that all types using them will definitely be declared, and since boundaries themselves
are not actual types that can be used in other types. But how do we represent them? Well, they're already well-represented enough in the boundary table, even if they're out-of-order. But we don't need them to be in lookup tables -- we just need arrays of the data that we
can iterate over simply, along with the names. So there's nothing special needed there.

If we want to compile the Boundary and Alias declarations into Kotlin, we can try, but we lack certain kinds of knowledge. In particular, we lack knowledge of the language. What _is_ Kotlin? What constructs are we interested in? And what does it mean to work with them? With Kotlin, we are interested in a few different things. First, we're interested in the representation of a Boundary as a Kotlin Interface. What does it mean to do so? Well, the
interface has:

- Name
- IsAsync
- Function*
  - Name
  - Arg*
    - Type*
  - Return
    - Type*
    
This is ... fairly easy, for most of it, to transform a Boundary into a Kotlin Interface data structure. The hard part is the `Type*`. This could be one of ... many things. But in terms of _data_ -- what Kotlin thinks it is -- it's quite simple. It's a 

```
{ name :: String, args :: Array String }
```

Surprisingly, that's all that's needed! A primary type name, and any type arguments that
are applied. Thus, an `Array Int` becomes a Kotlin `{ name: "List", args: [ Int ]}`, which
is written to file as `List<Int>`. Yet it's not that simple. Type arguments can be recursive, and so we have a Kotlin version of Param:

```
{ name :: String, args :: Array Param }
```

This recursive parameter can thus become a nested printout of a filled generic type. The next question is how we ultimately _transform_ the Params into Kotlin Params, but we'll table that discussion until after we know what Aliases are -- since Aliases will also need the answer to that question.

So for aliases, we have a name, and a concrete type. But not always. Sometimes the concrete type is really just a Param. In which case we go back to the discussion above. But it might also be a Record Map. In that case, we need to define a _Data Class_; that is,

- Name 
  - Param*
    - Name
    - Type*
    
That's it! Something is recursive about it. But the type itself might include Aliases ... so what really matters is that we can convert _any_ Param into the Kotlin type -- just the correct strings.

So how do we convert it? Well, we just start with our normal Param, and convert it to the KotlinParam. But to get the _actual_ type for Kotlin, we need the concept of Kotlin symbol lookup -- where each formal Symbol we have can find a lookup as a Kotlin type. We need ways to _ask questions_ ... but to ask them in a general way; to ask them in conceptual forms. Thus, we need different kinds of knowledge -- the Env, and a lookup table from simple strings from our application to Kotlin strings for OOB types. This is NOT the same as as aliases -- just the ability to transform Params to KotlinParams.

Okay. So now we can transform each of the "Kotlin" Params into a String. But the thing is ... they don't seem particularly like kotlin-specific parameters, or methods, or interfaces. They seem very-generalized representations of the final types in abstract form, and can be translated into any language. So there's no reason to keep them as they are. Instead, they each represent a generalized concept of a type -- an Interface, Method, Record, or Type parameter. And these are all we need to write them out; this intermediate representation is easy to convert to the target languages we're interested in. 
  But that means we have to answer the question of how each node of the Parse tree becomes the generalized form. And that varies by the _converter_ that is used. The converter is thus a unique type that specifically knows how to convert a _Param_ into a generalized param, since what needs replacing are the types used by the target language -- and that's all. It's more of a ... type-replacer, than anything else. After that, the conversion of each Generalized structure is defined per-structure for each language. Thus, our definition of each language defines a way to convert a Param to a GParam, and a way to turn each GStructure into a String (they rely on each other). And each GStructure defines, on its own, how to obtain itself from a Converter data structure, which is generalized and initialized from a language definition.
  
What we need right now... are 

- Ability to transform any files with extension .xxx found within a source directory, changing the extension but not the file name. So `Templates.tpurs` becomes `Templates.purs`.
- Ability to parse a template strings file into a bunch of template string definitions, for use in purescript.

Okay. So now we have to ability to pure purescript commands from the command-line. And we have the ability to run any shell command from purescript. So what does it mean to do the transformations that we want to do? How do we get to template strings?

- First, we need to make clearer the possible ways in which we can transform files, and fetch files for transformation. So for example, we should be able to query for _all_ files within a directory that match a given regex or extension, or contains something in the contents ala grep -- and get the absolute paths for all these files. That should be _easy_. Should be. It's certainly necessary. In other words, there's a need for a variety of ways to search for matching files, by a variety of parameters. And then once we have these paths, we should have a convenient way to _transform_ any set of files -- at minimum, to transform just by String to String -- and to specify the output file path. Taken together, this is per-file capability to Fetch, Convert, and Output -- transforming each file path either to the same file, or to a nearby file. That should be _easy_. 
- Just as easily, we should be able to select custom locations for each file, in case we want to recreate the file positions in a different directory entirely.
- We need to be able to _parse_ purescript files into data. It doesn't need to be an AST, but it _does_ need to be a reasonable representation of _what_ is present in the file, if we can parse it, in uncompiled form -- names of top-level symbols, their type signatures, and any literals they use. All of these things are sensible elements that we would want to know about, as data that we can analyze to generate _new_ strings.
- We might even want to go further -- and perform mapping of each element in the data representation to its position in the data, and enable _editing_ of each element, of some kind.
- Ability to _mark_ structures -- to add annotations. So just as Java has the ability to annotate a method, and just as Purescript can annotate code with comments:

```
-- | Comment
-- | Comment
```

We also want to be able to annotate a method of any other top-level declaration with one or more annotations:

```
--@Apple 1 2
--@Banana "hello" "goodbye"
```

This should be exposed in the data, so that folks can scan for annotations when reviewing functions, to determine if it needs further analysis. This is a decent use or comments, and helps provide additional data when scanning the code.

Together, the above provides for _markings_ that can become part of compile-time analysis, and help inform how the data turns into generated code.

- We need the ability to specify File Segments. This helps us handle the fact that even though we want to generate code, there are times when what we actually want is to _rewrite_ the current file, or _add_ to it, without overwriting it. For this to be true, we need delineated file segments, for example:

```
add :: Int -> Int -> Int
add = (+)

--SEG_START segmentName

subtract :: Int -> Int -> Int
subtract = (-)

--SEG_END segmentName
```

In this way, a file can be recognized as being composed of particular labeled segments. If we can easily add/remove these segments, we can generate code in the same file without touching unrelated code -- ideally, at the end of all user-written code, appended to the end of the file. In the ideal case, we'd start the transformation by DELETING all generated segments, and then adding the generated code back in during the build, from the data provided by the initial code.

File segments are thus ANOTHER way to transform a file -- `deleteSegment`, `appendSegment` and so on, as an abstraction for manipulating the code, if we're using segments.

As a result of the above, generating template strings becomes a relatively simple affair within a purescript file: We declare strings like the below, and generate a segment:


```
--@template-string
a :: String
a = "hello ${age :: Int}, world of ${name :: String} - ${name}!"

--SEG_START webb-template-strings
template_a :: { age :: Int, name :: String } -> String
template_a = ...
--SEG_END webb-template-strings
```

The conversion function would be autogenerated in segments at the end of the file, although it won't compile if the provided types don't have valid Show instances -- and we will probably use an 'isString' check to avoid calling 'show' on string types -- and the generation itself would fail if missing/conflicting types for a particular name were discovered. We are auto-generating, and can choose whether to convert _all_ names, all names of a particular explicit type, or all names with a particular annotation. And the _choice_ of what is generated would then depend on a BUILD function -- one (or more) that is specified in purescript, and executed using `webb-exec`.

This would remove the need for something like 

```
start a
hello ${age :: Int}, world of ${name :: String} - ${name}!
end a
```

We _could_ use the above as a DSL for a generator, but it's highly-specific and doesn't solve the general problem of turning Purescript into data -- any kind of data, really. It wouldn't, for example, solve the problem of generating helper methods for a render function:

```
--@props "render"
type Props = 
  { name :: String --@default 1
  , age :: String --@default 2
  , order :: String --@default 3
  , weight :: Int
  }
```

Based on those props, we might generate a functions and/or a monad just for the type, and a "RequiredProps" type that only has the `weight` parameter -- the rest are provided as defaults already. Likewise, in the new monad, we'd have functions specifically for setting parameters within the monad, and we'd have a `runMonad` function generated too -- all of it would be available, and ready for use, based on just one type definition, and re-generatable whenever you modify the annotations, if that's what you want.

We could even obtain public/private annotations to manually hide exports while generating the export list:

```
--@private 
a :: String
a = "2"

--@public
b :: String
b = "2"
```

The code generator could insert a segment at the top, automatically, exposing all types that are public or unmarked, and hiding any that are private, as part of the build process itself -- rewriting the module exports to only expose what the annotations allow. But the problem with relying on annotations for code transformation and code-generation is that the code itself might not be valid for it. A string marked "@template" might not contain the required string literal. It's not possible to do anything but log a warning in that case.

Granted, there are definitely namespace clashes here, and code-generators have no means of detecting the _source_ of a symbol, since this isn't a formal compilation process. But even this could be solved ... later, perhaps by annotation imports within comments at the top of the file, or annotations of the module itself to help determine how we want processing to work for each file -- some files could be processed differently than others, based on their annotations.

Thus, querying might even rely on reading the file and parsing it for its annotation, to determine which files to process.

TODO -- tokenizing. TypeName and FunctionName are last ... but they also match the keywords. So ... will that work out? Or do they have to be merged?